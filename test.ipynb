{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import random\n",
    "\n",
    "from IPython import display\n",
    "from skimage.draw import random_shapes, rectangle, polygon, circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_mask(size=(448, 448), max_shapes=3, show_result=False):\n",
    "    image, labels = random_shapes(size, min_shapes=1, max_shapes=max_shapes,\n",
    "                             min_size=size[0]/4, allow_overlap=True)\n",
    "\n",
    "    masks = []\n",
    "    for i in range(0, len(labels)):\n",
    "        img= np.zeros(size, dtype=np.uint8)\n",
    "\n",
    "        if(labels[i][0] == 'rectangle'):\n",
    "            rr, cc = rectangle((labels[i][1][0][0], labels[i][1][1][0]), (labels[i][1][0][1], labels[i][1][1][1]),\n",
    "                       shape=img.shape)\n",
    "        if(labels[i][0] == 'circle'):\n",
    "            y = labels[i][1][0][1]- (labels[i][1][0][1] - labels[i][1][0][0]) / 2\n",
    "            x = labels[i][1][1][1]-(labels[i][1][1][1] - labels[i][1][1][0]) / 2\n",
    "            r = (labels[i][1][0][1] - labels[i][1][0][0]) / 2        \n",
    "            rr, cc = circle(y, x, r, shape=img.shape)\n",
    "        if(labels[i][0] == 'triangle'):\n",
    "            x = (labels[i][1][1][0], labels[i][1][1][1] - (labels[i][1][1][1] - labels[i][1][1][0]) / 2, labels[i][1][1][1], labels[i][1][1][0])\n",
    "            y = (labels[i][1][0][1], labels[i][1][0][0], labels[i][1][0][1], labels[i][1][0][1])\n",
    "            rr, cc = polygon(y, x, shape=img.shape)                \n",
    "\n",
    "        img[rr, cc] = 1\n",
    "        masks.append(img)\n",
    "        \n",
    "    if(show_result):\n",
    "        print(labels)\n",
    "        fig=plt.figure(figsize=(8, 8))\n",
    "        fig.add_subplot(1, len(labels)+1, 1)\n",
    "        plt.imshow(image)\n",
    "        for i in range(0, len(labels)):    \n",
    "            fig.add_subplot(1, len(labels)+1, i+2)\n",
    "            plt.imshow(masks[i], cmap=\"Greys\")\n",
    "        plt.show()\n",
    "    return image, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(raw_image):\n",
    "    t = tf.convert_to_tensor(image, np.float32) / 255.0\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "masks = []\n",
    "IMAGE_COUNT = 300\n",
    "for i in range(0, IMAGE_COUNT):\n",
    "    image, mask = generate_image_mask(size=(224, 224), max_shapes=1)\n",
    "    images.append(preprocess_image(image))\n",
    "    masks.append(preprocess_image(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ds = tf.data.Dataset.from_tensor_slices(images)\n",
    "msk_ds = tf.data.Dataset.from_tensor_slices(masks)\n",
    "img_msk_ds = tf.data.Dataset.zip((img_ds, msk_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 224, 224, 3), (None, 224, 224, 3)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Setting a shuffle buffer size as large as the dataset ensures that the data is\n",
    "# completely shuffled.\n",
    "ds = img_msk_ds.apply(\n",
    "  tf.data.experimental.shuffle_and_repeat(buffer_size=IMAGE_COUNT))\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    #model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    #assert model.output_shape == (None, 28, 28, 32)\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    \n",
    "    #model.add(layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    #assert model.output_shape == (None, 56, 56, 16)\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    \n",
    "    #model.add(layers.Conv2DTranspose(8, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    #assert model.output_shape == (None, 112, 112, 8)\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    #model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    #assert model.output_shape == (None, 224, 224, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1de61212908>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGJ9JREFUeJzt3XuM1dW1B/DvYhjej4LAMDAgD0cFsYId33pRGnwVozbVVmvFxJYmtUZJm9hqm5q0NzE3t3r7x01buNBKKlUqVC0xWnwFqA9ERBRRoDDyGhjkUQehwMC6f3DsHZH9XcPMcM549/eTmBnOlz1nc+Ysz8zZv722uTtEJD8dSj0BESkNFb9IplT8IplS8YtkSsUvkikVv0imVPwimVLxi2RKxS+SqY7FvLOuXbt67969k7mZ0fGHDx9u6yk1W1lZWTLbu3cvHdupUyeal5eX03z//v00P3DgQDLr2rUrHdvY2Ejzzp07t/i+AaBLly7J7ODBg3RsJHrcu3fvnsyi51pk3759NI++5+zK2o4deVkeOnQomTU0NGDfvn3N+se1qvjN7EoAvwJQBuB/3P0B9vd79+6NW265JZmzJwrAv9nsAQHib3aHDvyHoF69eiWz5cuX07FDhgyh+YABA2i+du1amtfV1SWzMWPG0LEffvghzYcPH07z2tpamo8aNSqZsXkD/H+4ALBs2TKan3POOS3+2tHzZdWqVTSvrKykOXsh69+/Px27c+fOZPb444/TsU21+Md+MysD8N8ArgIwGsBNZja6pV9PRIqrNb/znwtgrbuvc/cDAB4FcG3bTEtETrTWFP9gABub/HlT4bZPMbMpZrbUzJZGv6OJSPG0pviP9UvRZ97FcPdp7l7j7jXdunVrxd2JSFtqTfFvAtD0nawqAFtaNx0RKZbWFP/rAKrNbLiZdQLwDQBPtc20ROREa/FSn7s3mtn3ATyLI0t9M919ZTCGritH7wmw5bg9e/bQsYMHf+btiE/ZunUrzU877bRkFq2Vn3zyyTTftWsXzf/5z3/SvKqqKplFa8annnoqzdl1GQDwwQcf0Jyth3/ta1+jYzdv3kzzaHmWLaE2NDTQsdFz8bzzzqN5v379aM6eM6tXr6Zj2fMpur6gqVat87v70wCebs3XEJHS0OW9IplS8YtkSsUvkikVv0imVPwimVLxi2SqqPv5O3ToQLftbtiwgY5na/XRXv/t27fTPNq3/re//S2Z9ejRg46N1un79u1L8+rqaprv3r07mUXr2dGWXrZ9FIh7DbC19lmzZtGx9fX1NL/wwgtpzq5BaE2PBADYuHEjzSMjRoxIZtFa/bZt25LZ8fRI0Cu/SKZU/CKZUvGLZErFL5IpFb9IplT8Ipkq6lJfeXk5Xa6LOp6yJY5LLrmEjo2WvD766COasw68bF5AvGQVdR6O8gkTJiSzaEtutH10yxbenyXqPPz2228ns9NPP52OjbZCR9u4WRfcdevW0bHXXXcdzVv7PR84cGAyi74n7N99PO3t9covkikVv0imVPwimVLxi2RKxS+SKRW/SKZU/CKZKuo6/8GDB+nJrNGWXrbVMWqfHZ10G60psyOVo22x7IRfAHj11VdpHm0JZmv50Vp4dDJydFpttCb9j3/8I5mxxxSItxO/9957NB80aFAyi9p+L1q0iObR9zw6HXnFihXJLDqynV2TEl0T0pRe+UUypeIXyZSKXyRTKn6RTKn4RTKl4hfJlIpfJFOtWuc3s1oADQAOAWh09xr2992drkOOHz+e3h9rxTx27Fg6trVHLs+bNy+ZDR06lI6N9sRHa8Jdu3al+aRJk5IZ208PxHNbvHgxzW+99Vaas/Xsbt260bHROv9Pf/pTmrNrHKJ/V/S4jBo1iuZRK3j2fGXXJwC8F8HLL79MxzbVFhf5XObu/IoHEWl39GO/SKZaW/wO4K9m9oaZTWmLCYlIcbT2x/6L3H2LmQ0AsMDM3nP3hU3/QuF/ClOA+Bp3ESmeVr3yu/uWwsd6AH8GcO4x/s40d69x95rojSsRKZ4WF7+ZdTeznp98DuByAO+01cRE5MRqzY/9FQD+XNjy2RHAbHd/pk1mJSInXIuL393XATjreMaUlZWhZ8+eyfzZZ5+l49l7BtHRxNGa8bvvvktzdkT3RRddRMdGe7/ZmQAAsGPHDpr/9re/TWbReQXdu3enedT3f8mSJTRn687RWQvRfd933300Z/0fvvKVr9Cxa9asofns2bNpfsUVV9D8ySefTGZXX301Hfvxxx8ns3379tGxTWmpTyRTKn6RTKn4RTKl4hfJlIpfJFMqfpFMWdQ+uS0NGTLEp06dmszZEgYA7N+/P5lVV1fTsVFL461bt7Z4fLRdePjw4TQ/6aSTaL5y5UqaT5w4MZlF/64333yT5pHomG22pThaIo2O0Y7airNjstmSMxAfF9+nTx+a79q1i+ZLly5NZmVlZXQsO5J96tSpWLNmDX9gCvTKL5IpFb9IplT8IplS8YtkSsUvkikVv0imVPwimSrqEd379+/H+vXrk3m0nj1gwIBk1r9/fzp206ZNNI9aXJ9yyinJLDoGm12fAACvvPIKzaNrMdia8RlnnEHHRluho6PNWWtuAKipSXdzX7ZsGR0bXYMQXWPw/vvvJ7NrrrmGjn3rrbdovn37dppHx6qzaxRGjx5Nx/7pT39KZtH1BU3plV8kUyp+kUyp+EUypeIXyZSKXyRTKn6RTKn4RTJV1HV+M0PHjum7PPPMM+n4vn37JrNovXrjxo00j47JZuvZN9xwAx0btceO2i1HJx2df/75yayuro6OnT9/Ps2jNtLsvgFgzpw5ySx6XG6//XaaR8don3VWurN81E69Qwf+usi+NhDPjT2uM2fOpGNZW/CFCxcms6PplV8kUyp+kUyp+EUypeIXyZSKXyRTKn6RTKn4RTIV9u03s5kAJgGod/cxhdv6AngMwDAAtQBudPdwI3FFRYV/85vfTObRHmm29lpRUUHHjho1iua/+c1vaM5663fu3JmOLS8vp3m05vzRRx/RnJ0pED0u0dwHDx5M8w0bNtCc9UGI+hh069aN5gMHDqQ5e25Hx38fPnyY5tE5ELW1tTRn16xE12ZUVVUlswULFmDnzp1t1rf/9wCuPOq2HwF43t2rATxf+LOIfI6Exe/uCwHsPOrmawE8XPj8YQDXtfG8ROQEa+nv/BXuXgcAhY/p/loi0i6d8Df8zGyKmS01s6XRNewiUjwtLf5tZlYJAIWPyRMR3X2au9e4e020QUVEiqelxf8UgMmFzycDeLJtpiMixRIWv5n9EcArAE4zs01mdjuABwBMNLM1ACYW/iwinyPhOn9bqqqq8jvvvDOZR+fUs/PWe/XqRcdu3ryZ5oMGDaI5e78i6h//wgsv0DzqJRCtZ7O1/OhMgUWLFtE8etzuvfdemr/44ovJLLrGYNiwYTRnzwcA2LZtWzKLHvMFCxbQ/OOPP6Z5dH0Fy9k5DAC/RmH69OnYsmVLm63zi8j/Qyp+kUyp+EUypeIXyZSKXyRTKn6RTBW1dffBgwfp0lG0tXXx4sXJLGoxvWbNGppHS2INDQ3JLDree8iQITT/y1/+QvNoS3B1dXUyi46a3rFjB80PHDhA84ceeojm7IjvqD12dLR5tNX5vvvuS2aPPfYYHRsdsR21go+2OrMtwdGyM/ueHM/SvV75RTKl4hfJlIpfJFMqfpFMqfhFMqXiF8mUil8kU0Vd53d3NDY2JnPWzhgAJk6cmMzWr19Px44fP57m0drqgw8+mMxmzJhBx0bHJkfbQ6P1cLZmfMstt9CxU6dOpfl5551H8wkTJtCcXeNwwQUX0LFPPPEEzaPjwV999dVkNnbsWDr29ddfp/n1119P86gN/bhx45LZ7Nmz6djKyspkVlZWRsc2pVd+kUyp+EUypeIXyZSKXyRTKn6RTKn4RTKl4hfJVFHX+c0MXbp0SeabNm2i43fuPPq80P8T7e3u1KkTzVevXk1z9vXnzZtHx0Z9CliLaQC47LLLaP7aa68ls3Xr1tGxl1xyCc2vvPLoA5o/bebMmTRn++KfeeYZOpathQO8vwPA/+3R97tHjx40nzt3Ls1Hjx5Nc9YaPJrbyJEjk1l0tHhTeuUXyZSKXyRTKn6RTKn4RTKl4hfJlIpfJFMqfpFMhev8ZjYTwCQA9e4+pnDb/QC+A+CTTcv3uvvT0dfq1KkT3d8d9UofOnRoMmPXDwDA7t27ad6xI38o2N7xm2++mY6NjnseMWIEzaNeBXfddVcyY0eLA3F/+fnz59P81ltvpTnbUx+dpRAd2X7WWWfRnJ138Mgjj9Cxp556Ks2jXgTdu3en+XPPPZfMzj33XDqW9b2InsdNNeeV//cAjnWlx0PuPrbwX1j4ItK+hMXv7gsBpC+tE5HPpdb8zv99M1thZjPNrE+bzUhEiqKlxf9rACMBjAVQB+CXqb9oZlPMbKmZLY161YlI8bSo+N19m7sfcvfDAKYDSL5D4e7T3L3G3WuiN0FEpHhaVPxm1rR96PUA3mmb6YhIsTRnqe+PAC4F0M/MNgH4GYBLzWwsAAdQC+C7J3COInIChMXv7jcd42beqD6hsbGR7l2P9rXv2rUrmUX95dlYABg4cCDNe/funcx++MMf0rHV1dU0j86hZ2fcA8ADDzyQzCoqKujYYcOG0TyaW3SdADunoXPnznRs1Cdhy5YtNO/Xr18yW7JkCR3L5g3EZymwPfcAfz5GvSl27NiRzI7nfTVd4SeSKRW/SKZU/CKZUvGLZErFL5IpFb9Iporauvvw4cPYu3dvMu/Th28R+NKXvpTMamtr6Vh2jDUQH9HNxvfq1YuOPXDgAM2rqqpoHi3fnH766TRnoq2tUevu+vp6mrOt1u+99x4dGx3Z3tDQQHP2fIpac0f3HT1Xoxba0RIrM2XKlGS2YsWKZn8dvfKLZErFL5IpFb9IplT8IplS8YtkSsUvkikVv0imirrOX15eTtfToy29Tz+dbhLs7nSsmdF82bJlNGdr8RdffDEd27VrV5pHrbm3b99Oc7Y9NHpcvvrVr9I82voaHUXN2rFH240jK1eupDnrHBW1uN6zZ0+r7nvChAk0Z627oyPZn3/++WQWbQduSq/8IplS8YtkSsUvkikVv0imVPwimVLxi2RKxS+SqaKu83fs2JEeu8xaLQP82OQvfOELdCxbGwWAsWPH0pytn9bU1NCxURvo6DqA6N924YUXJrPolKRf/OIXNO/WrRvNo2O0v/zlLyezv//973TsK6+8QvM77riD5m+88UYyi66dYH0nAODb3/42zVevXk3zr3/968ks6nPAeihEz7Wm9MovkikVv0imVPwimVLxi2RKxS+SKRW/SKZU/CKZsmbsgx8CYBaAgQAOA5jm7r8ys74AHgMwDEAtgBvdnZ6D3a9fP580aVIyHzFiBJ0L22O9atUqOpathQNx33+293zr1q10bNTD/YMPPqB5z549aT5gwIAWZUB8jUF03x9++CHNy8rKktn06dPp2P79+9M8OmuBnXcQHcl+8OBBmo8fP57m0ePy5ptvJrPoucquf3jppZewe/du3ryioDmv/I0AfuDuowCcD+AOMxsN4EcAnnf3agDPF/4sIp8TYfG7e527Lyt83gBgFYDBAK4F8HDhrz0M4LoTNUkRaXvH9Tu/mQ0DMA7AawAq3L0OOPI/CAD850sRaVeaXfxm1gPAXAB3u3uzG4WZ2RQzW2pmS1k/NxEprmYVv5mV40jhP+Lu8wo3bzOzykJeCeCYuw3cfZq717h7DTu0UUSKKyx+O9L2dgaAVe7+YJPoKQCTC59PBvBk209PRE6U5iz1XQxgEYC3cWSpDwDuxZHf++cAGApgA4Ab3H0n+1pVVVV+5513JvPevXvTuWzevDmZnXLKKXRsXV0dzaNjrt96661kVl1dTccuXryY5tGvQ9ES6E9+8pNkds8999CxCxcupPnQoUNp/q1vfYvmrB179P1mrdoB4OWXX6b5z3/+82R244030rFXXXUVzb/3ve/R/IknnqD52rVrk1m0hMnyP/zhD9i6dWuzlvrC/fzuvhhA6oulN2uLSLumK/xEMqXiF8mUil8kUyp+kUyp+EUypeIXyVRRW3c3NjbSlskvvPACHb9///5kFrWojq5nmDVrFs3HjRuXzGbOnEnHRsc1z5kzh+adO3em+Y9//OMWjz1w4ADNoyOf586dS3NmzJgxNI+21UZr7WwL+E033UTHRkd4s2tOgPjajWuuuSaZvfjii3Qsa+UebR9vSq/8IplS8YtkSsUvkikVv0imVPwimVLxi2RKxS+SqaKu85eXl6OysjKZn3zyyXQ8a4cctWKOjv+O1mVZC+ozzjiDjo3WjC+//HKaR629zzzzzGQW9QKI1tpfeuklml9wwQU037dvXzJbsmQJHdujRw+aX3311TSfMWNGMose80cffZTm0fPlsssuo/lzzz2XzKJj0S+99NJk9uyzz9KxTemVXyRTKn6RTKn4RTKl4hfJlIpfJFMqfpFMqfhFMhX27W9LlZWVPnny5GReX3/MQ3/+ha1/Rr3zo/3Xy5Yto/nIkSOT2c6d9LiC8EyA+fPn0zzqnd+3b99kFj0uixYtovmhQ4dofvPNN9N81670qe1Rr4BNmzbRPJrb6NGjk9nvfvc7Oja6hiA6U+Dss8+m+eDBg5NZY2MjHfv+++8ns0WLFrXpEd0i8v+Qil8kUyp+kUyp+EUypeIXyZSKXyRTKn6RTIX7+c1sCIBZAAYCOAxgmrv/yszuB/AdAJ804r/X3enip7vTvuJRD3l2TUJDQwMdy/a8A7xXAAD06tUrmUV72nfs2EHzUaNG0TyaO+uD8Mwzz9CxFRUVNC8vL6d5VVUVzVkvgt27d9Ox0Vr63XffTXP2Pbv22mvp2C5dutD8tttuo3nUP4KdQbF8+XI69pxzzklm0fUqTTWnmUcjgB+4+zIz6wngDTNbUMgecvf/bPa9iUi7ERa/u9cBqCt83mBmqwCkL08Skc+F4/qd38yGARgH4LXCTd83sxVmNtPM+iTGTDGzpWa2lLV0EpHianbxm1kPAHMB3O3uHwH4NYCRAMbiyE8GvzzWOHef5u417l7TtWvXNpiyiLSFZhW/mZXjSOE/4u7zAMDdt7n7IXc/DGA6gHNP3DRFpK2FxW9mBmAGgFXu/mCT25u24b0ewDttPz0ROVHCLb1mdjGARQDexpGlPgC4F8BNOPIjvwOoBfDdwpuDSYMGDfIpU6Yk8w0bNtC59O/fP5lt2bKFju3T55hvSfzL+vXrac629K5cuZKOjVoxs3bmAD9qGuCtwaOW5itWrKB5M54fNGfLbV/84hfp2LVr19I8Wk5jW4KHDRtGx9bV0acyXaoDgK1bt9KcLXlHy6fsMX388cdRX1/frC29zXm3fzGAY30xvggrIu2arvATyZSKXyRTKn6RTKn4RTKl4hfJlIpfJFNFPaL78OHD2Lt3bzKPtlGyVs/RkcvRenR0HHTPnj2TWbTtNTrOOdrzEK3zs2O2o3X+d97h12ZF242jY9UPHjyYzKLrOqLtxlE7dtY+O7qG4KSTTqJ5dGz6FVdcQfNt27Yls+j5MGjQoGQWPReb0iu/SKZU/CKZUvGLZErFL5IpFb9IplT8IplS8YtkqqhHdJvZdgBNF0j7AeA9s0unvc6tvc4L0Nxaqi3ndrK7pxtfNFHU4v/MnZstdfeakk2AaK9za6/zAjS3lirV3PRjv0imVPwimSp18U8r8f0z7XVu7XVegObWUiWZW0l/5xeR0in1K7+IlEhJit/MrjSz981srZn9qBRzSDGzWjN728yWm9nSEs9lppnVm9k7TW7ra2YLzGxN4SPvSV7cud1vZpsLj91yM7u6RHMbYmYvmtkqM1tpZncVbi/pY0fmVZLHreg/9ptZGYDVACYC2ATgdQA3ufu7RZ1IgpnVAqhx95KvCZvZvwHYA2CWu48p3PYfAHa6+wOF/3H2cfd72snc7gewp9QnNxcOlKlserI0gOsA3IYSPnZkXjeiBI9bKV75zwWw1t3XufsBAI8C4IelZ8rdFwLYedTN1wJ4uPD5wzjy5Cm6xNzaBXevc/dlhc8bAHxysnRJHzsyr5IoRfEPBrCxyZ83oX0d+e0A/mpmb5hZ+nih0qn45GSkwscBJZ7P0cKTm4vpqJOl281j15ITr9taKYr/WP202tOSw0XufjaAqwDcUfjxVpqnWSc3F8sxTpZuF1p64nVbK0XxbwIwpMmfqwDwg/aKyN23FD7WA/gz2t/pw9s+OSS18LG+xPP5l/Z0cvOxTpZGO3js2tOJ16Uo/tcBVJvZcDPrBOAbAJ4qwTw+w8y6F96IgZl1B3A52t/pw08BmFz4fDKAJ0s4l09pLyc3p06WRokfu/Z24nVJLvIpLGX8F4AyADPd/d+LPoljMLMROPJqDxzpbDy7lHMzsz8CuBRHdn1tA/AzAE8AmANgKIANAG5w96K/8ZaY26U4zpObT9DcUidLv4YSPnZteeJ1m8xHV/iJ5ElX+IlkSsUvkikVv0imVPwimVLxi2RKxS+SKRW/SKZU/CKZ+l/Zw344qWDwtgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))    \n",
    "    \n",
    "    #model.add(layers.Conv2D(512, (5, 5), strides=(2, 2), padding='same'))\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #model.add(layers.Conv2D(1024, (5, 5), strides=(2, 2), padding='same'))\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))        \n",
    "    \n",
    "    #model.add(layers.Conv2D(2048, (5, 5), strides=(2, 2), padding='same'))\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #model.add(layers.Conv2D(4096, (5, 5), strides=(2, 2), padding='same'))\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00373178]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as we go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds1 = img_ds.apply(\n",
    "  tf.data.experimental.shuffle_and_repeat(buffer_size=IMAGE_COUNT))\n",
    "ds1 = ds1.batch(BATCH_SIZE)\n",
    "\n",
    "train(ds1, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
