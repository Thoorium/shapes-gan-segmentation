{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import random\n",
    "\n",
    "from IPython import display\n",
    "from skimage.draw import random_shapes, rectangle, polygon, circle\n",
    "\n",
    "tf.config.gpu.set_per_process_memory_growth(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE_SIZE = 28\n",
    "IMAGE_COUNT = 600\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_mask(size=(448, 448), max_shapes=3, show_result=False):\n",
    "    image, labels = random_shapes(size, min_shapes=1, max_shapes=max_shapes,\n",
    "                             min_size=size[0]/4, allow_overlap=True, multichannel=False, shape='triangle')\n",
    "\n",
    "    masks = []\n",
    "    for i in range(0, len(labels)):\n",
    "        img= np.zeros(size, dtype=np.uint8)\n",
    "\n",
    "        if(labels[i][0] == 'rectangle'):\n",
    "            rr, cc = rectangle((labels[i][1][0][0], labels[i][1][1][0]), (labels[i][1][0][1], labels[i][1][1][1]),\n",
    "                       shape=img.shape)\n",
    "        if(labels[i][0] == 'circle'):\n",
    "            y = labels[i][1][0][1]- (labels[i][1][0][1] - labels[i][1][0][0]) / 2\n",
    "            x = labels[i][1][1][1]-(labels[i][1][1][1] - labels[i][1][1][0]) / 2\n",
    "            r = (labels[i][1][0][1] - labels[i][1][0][0]) / 2        \n",
    "            rr, cc = circle(y, x, r, shape=img.shape)\n",
    "        if(labels[i][0] == 'triangle'):\n",
    "            x = (labels[i][1][1][0], labels[i][1][1][1] - (labels[i][1][1][1] - labels[i][1][1][0]) / 2, labels[i][1][1][1], labels[i][1][1][0])\n",
    "            y = (labels[i][1][0][1], labels[i][1][0][0], labels[i][1][0][1], labels[i][1][0][1])\n",
    "            rr, cc = polygon(y, x, shape=img.shape)                \n",
    "\n",
    "        img[rr, cc] = 1\n",
    "        masks.append(img)\n",
    "        \n",
    "    if(show_result):\n",
    "        print(labels)\n",
    "        fig=plt.figure(figsize=(8, 8))\n",
    "        fig.add_subplot(1, len(labels)+1, 1)\n",
    "        plt.imshow(image)\n",
    "        for i in range(0, len(labels)):    \n",
    "            fig.add_subplot(1, len(labels)+1, i+2)\n",
    "            plt.imshow(masks[i], cmap=\"Greys\")\n",
    "        plt.show()\n",
    "    return image, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(raw_image):\n",
    "    t = tf.convert_to_tensor(raw_image, np.float32)\n",
    "    t = tf.reshape(t, (SHAPE_SIZE, SHAPE_SIZE, 1,))\n",
    "    t = (t - 127.5) / 127.5\n",
    "    return t\n",
    "\n",
    "def preprocess_mask(raw_mask):\n",
    "    t = tf.convert_to_tensor(raw_mask, np.float32)\n",
    "    #t = tf.dense(t, (784))\n",
    "    t = tf.reshape(t, (SHAPE_SIZE, SHAPE_SIZE, 1,))\n",
    "    t = (t - 127.5) / 127.5\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "60\n",
      "120\n",
      "180\n",
      "240\n",
      "300\n",
      "360\n",
      "420\n",
      "480\n",
      "540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c290c96a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = []\n",
    "masks = []\n",
    "for i in range(0, IMAGE_COUNT):\n",
    "    image, img_masks = generate_image_mask(size=(SHAPE_SIZE, SHAPE_SIZE), max_shapes=1)\n",
    "    images.append(preprocess_image(image))\n",
    "    masks.append(preprocess_mask(img_masks[0]))\n",
    "    if((i / IMAGE_COUNT * 100.0) % 10 == 0):\n",
    "        print(i)\n",
    "# Display a sample image\n",
    "plt.imshow(images[0][:, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "#plt.imshow(images[0][:, :] * 127.5 + 127.5, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ds = tf.data.Dataset.from_tensor_slices(images)\n",
    "msk_ds = tf.data.Dataset.from_tensor_slices(masks)\n",
    "img_msk_ds = tf.data.Dataset.zip((img_ds, msk_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 28, 28, 1), (None, 28, 28, 1)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting a shuffle buffer size as large as the dataset ensures that the data is\n",
    "# completely shuffled.\n",
    "#ds = img_msk_ds.apply(\n",
    "#  tf.data.experimental.shuffle_and_repeat(buffer_size=IMAGE_COUNT))\n",
    "#ds = ds.batch(BATCH_SIZE)\n",
    "#ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "#ds\n",
    "\n",
    "# Setting a shuffle buffer size as large as the dataset ensures that the data is\n",
    "# completely shuffled.\n",
    "ds = img_msk_ds.shuffle(buffer_size=IMAGE_COUNT)\n",
    "#ds = ds.repeat()\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "# `prefetch` lets the dataset fetch batches, in the background while the model is training.\n",
    "ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(28,28,1,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Reshape((28, 28, 256)))\n",
    "    #assert model.output_shape == (None, 28, 28, 256) # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    #assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    #model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    #assert model.output_shape == (None, 14, 14, 64)\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    \n",
    "    #model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    #assert model.output_shape == (None, 28, 28, 32)\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    \n",
    "    #model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    #assert model.output_shape == (None, 56, 56, 16)\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    #assert model.output_shape == (None, 56, 56, 16)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    ########\n",
    "    #model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    #assert model.output_shape == (None, 56, 56, 16)\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    \n",
    "    #model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    #assert model.output_shape == (None, 112, 112, 8)\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    #model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    #assert model.output_shape == (None, 224, 224, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c2a15d710>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGU1JREFUeJzt3Xlw1dXZB/DvAwZBFMuSsMguVgRsUSKutC4g+I6IgjJC6yCCtB0pODodraMjbcfWOmqVqYOiUAG1GhUotG4oWLC+I0RkAA2IC0gMEHaQfXneP7i8g8r5npiEe2PP9zPDEPLNk3vySx5ukvM755i7Q0TSUyvXAxCR3FDziyRKzS+SKDW/SKLU/CKJUvOLJErNL5IoNb9IotT8Iok6LpsPVq9ePW/QoEEwj91tyPKDBw/S2n379sXGRvMDBw4EMzOjtTHsfQNA7dq1aZ6XlxfMdu3aRWuPP/54msfGFvuc7d+/P5ideOKJtHb37t00r1WLP3exsdepU4fW7tmzh+bsmgPxr0eGXbPYY2/fvh27du2q0BdklZrfzPoAeBRAbQBPufv97O0bNGiAwYMHB/PYBWcNvGPHDlq7YcMGmp9xxhk03759ezCLfSHFGoi9bwD4wQ9+QPOCgoJgtnTpUlrboUMHmm/evJnmsY9t06ZNwezcc8+ltcuWLaN5/fr1ac6ua5s2bWjtihUraN6iRQuax/7TZddt48aNlX7soqIiWnukSn/bb2a1ATwG4AoAnQAMMrNOlX1/IpJdVfmZvzuAT9z9M3ffC+B5AP2qZ1gicqxVpflPAbD6iH+XZl73NWY2wsyKzaw49q2QiGRPVZr/aL9U+NZvf9x9vLsXunth7JdqIpI9VWn+UgCtjvh3SwBlVRuOiGRLVZp/AYDTzKydmdUBcD2AGdUzLBE51io91efu+81sJIDXcWiqb6K7f8hq6tSpg1atWgXzd999lz7mjTfeGMzee+89WnvBBRfQ/JNPPqH5pZdeGsymTp1Kay+55BKaP/744zT/9a9/TXM2Bdq/f39a+8ILL9CcXXMAmDRpEs1/+9vfBrOnn36a1nbr1o3md999N82HDRsWzJYvX05r+/TpQ/Px48fT/MEHH6T5Aw88EMy6du1Ka9n0bex+liNVaZ7f3V8B8EpV3oeI5IZu7xVJlJpfJFFqfpFEqflFEqXmF0mUml8kUZbNE3saN27svXv3DuannXYarWfLQ+vWrUtrmzdvTnO2LBYAZs+eHczat29Paz/77DOan3nmmTRfvXo1zRs1ahTMtm3bRmtjS5nnz59P844dO9KcrefYsmULrT3hhBNofuGFF9L8n//8ZzD78Y9/TGtLSkpovnjxYpqfd955NGdr9mN9sHbt2mA2duxYlJaWVmg9v575RRKl5hdJlJpfJFFqfpFEqflFEqXmF0lUVrfurl+/Ps4///xgHlsa+/DDDwezl19+mdbGpk/Gjh1L8/vvD29M/NBDD9HaXr160fzZZ5+l+cCBA2n+9ttvB7NRo0bRWnZNAeCee+6h+ejRo2nOdh5+9dVXaW3susauW79+4S0lY0uRCwsLaT506FCa79y5k+bPPfdcMFu/fj2tPe64cNvu3buX1h5Jz/wiiVLziyRKzS+SKDW/SKLU/CKJUvOLJErNL5KorC7pPfHEE51tS3z22WfTejZ3GluyGzuKms2dxh47dmLrnDlzaH7rrbfSfNy4cTRnJxD36NGD1saW5L744os0Hz58OM1nzZoVzGLXPHYCcGyL6yVLlgSzzp0701q2bBaIX7fYPQhsGfdHH31Ea/Pz84PZlClTsHbtWi3pFZEwNb9IotT8IolS84skSs0vkig1v0ii1PwiiarSPL+ZrQSwHcABAPvdnS6Cbteund97773BfMKECfTx2Lbf//rXv2ht06ZNaR6bq7/yyiuD2Z/+9Cdayz5mIL6mfuTIkTR/5513glmTJk1obWxb8JjYfHjPnj2D2eTJk2lt7L6PdevW0Xzw4MHBbMGCBbS2Xr16NG/bti3N2dbcAPDmm28Gs1q1+HNyXl5eMJs6dSrWr19foXn+6tjM4xJ3D99lIiI1kr7tF0lUVZvfAbxhZu+b2YjqGJCIZEdVv+2/0N3LzKwAwCwzW+buc498g8x/CiMAoHHjxlV8OBGpLlV65nf3sszf5QCmAeh+lLcZ7+6F7l540kknVeXhRKQaVbr5zay+mZ10+GUAlwNYWl0DE5Fjqyrf9jcFMM3MDr+f59z9tWoZlYgcc1ldz19QUODXXnttMI/trb9w4cJg1q1bN1r75Zdf0vymm26i+QMPPBDMmjVrRmtXrlxJ85YtW9K8vLyc5uyY7dh882WXXUbzF154gebt2rWjOTuGO1b71ltv0Ty2pp4djR7b/yH2I2pZWRnNY7/fKi0tDWaxI9937NgRzJ588kmUlZVpPb+IhKn5RRKl5hdJlJpfJFFqfpFEqflFEpXVI7rNDHXr1g3ms2fPpvX33XdfMJs+fTqtZY8LALfffjvNx4wZE8xiS1NPP/10mn/88cc0LygooDlb4nnRRRfR2tgR3uyYawDYvXs3zU8++eRgtnXrVlrLjkUHgGuuuYbmbPvs2NRubOq4e/dv3cz6NYsXL6b5VVddFczYseYAP8r+4MGDtPZIeuYXSZSaXyRRan6RRKn5RRKl5hdJlJpfJFFqfpFEZXVJb35+vrO52S5dutD6ZcuWBTM2nwzEt7A+99xzac7m8jt06EBrY3OvtWvXpvmePXtozpbNbt++ndY2bNiQ5rFtpNljA3yZ9rZt22jtqlWraB5bCs3unxg6dCitjR1N3qpVK5pv3ryZ5q1btw5my5cvp7X79u0LZjNmzMCGDRu0pFdEwtT8IolS84skSs0vkig1v0ii1PwiiVLziyQqq/P8rVu39t/85jfBfObMmbT+hhtuCGa///3vae3w4cNp/umnn9K8b9++weypp56itbfeeivNY0eTx46Lzs/PD2b9+/entbGjzdm6cwD4wx/+QPM77rgjmJWUlNDaOnXq0JwdTQ4AAwYMCGaPPfYYrY3dQzBv3jyaDxo0iObsyPjYduts74p58+Zhy5YtmucXkTA1v0ii1PwiiVLziyRKzS+SKDW/SKLU/CKJis7zm9lEAFcCKHf3LpnXNQLwAoC2AFYCGOjufAEzgCZNmjibN77gggto/YIFC4JZbD1/bO34wIEDac6OB48d0b1kyRKax8b+wQcf0Pz6668PZrEjtmMfd+yYbHY8OMA/Z7E18Xv37qV57DwEtr99jx49aO3OnTtpHltzH/vYzjnnnGDGvtYA4P333w9mxcXF2LZtW7XN8z8NoM83XncngLfc/TQAb2X+LSLfI9Hmd/e5ADZ949X9AEzKvDwJwNXVPC4ROcYq+zN/U3dfAwCZv/l5UiJS4xzzX/iZ2QgzKzaz4ti5biKSPZVt/nVm1hwAMn+Xh97Q3ce7e6G7F8YOyxSR7Kls888AMCTz8hAA/6ie4YhItkSb38z+DuB/AZxuZqVmNgzA/QB6mdkKAL0y/xaR75Gs79vPznvftOmbkwpfx/aAr1+/Pq1t1KgRzd9++22a33LLLcEstnd97Lz12NryPn2+OdP6dR9++GEwi12X9evX0zw2H/7EE0/QvHfv3sGsrKyM1nbr1o3mM2bMoPltt90WzB5++GFay+bhAaBBgwY0j+0P0bZt22AW+/G4vDz4UzaeeOIJlJWVaT2/iISp+UUSpeYXSZSaXyRRan6RRKn5RRKV1am+xo0bO5v6YVtQA/xo4pNOOonWLlq0iOZDhgyheWlpaTBjR0EDwA9/+EOax45zjk0VsmW1bPknAKxZs4bm7OMG+JbmAF+eGpvKiy1ljk1jNm/ePJh99dVXtHbr1q00jx3p/vnnn9P8Jz/5STBjS5EB/vUyc+ZMHdEtIpyaXyRRan6RRKn5RRKl5hdJlJpfJFFqfpFEHZfNB8vLy6NbGq9evZrWDx48OJj96le/orX33HMPzWNbex88eDCYsflkAOjSpQvN58yZQ/PLL7+c5o888kgwa9GiBa2NHYN9ySWX0Hzy5Mk0P//884PZ4sWLaW1sHj92lDVb6hzbUu7OO/mG1Pfddx/N2RHcAPD8888Hs9jYzCo0jR+lZ36RRKn5RRKl5hdJlJpfJFFqfpFEqflFEqXmF0lUVtfzN2vWzNm6+dj22tu3bw9mse2O586dS/NRo0bR/MUXXwxmsSO2jzuO304Rm9eNzcWfcsopNGdic+07duygeefOnWm+dOnSYFavXj1aW1DAj4CM7ZPAHju2R0JJSQnNY/sBdOzYkeZt2rQJZrHPCaudMGGCtu4WEU7NL5IoNb9IotT8IolS84skSs0vkig1v0iiouv5zWwigCsBlLt7l8zrxgC4GcDh853vcvdXYu9r9+7dWL58eTCPrR1fu3ZtMDtw4ACtLSoqonlsP4CJEycGs/79+9Pa2HHOn332Gc179uxJc7au/YQTTqC1bL09ALr/AhDfn37AgAHBjH0tAMC1115L83fffZfmbC4/dhz8ddddR/Np06bRPLaen33OmzRpQmvZWQvsbItvqsgz/9MAjnZA/F/cvWvmT7TxRaRmiTa/u88FwP+bFJHvnar8zD/SzBab2UQza1htIxKRrKhs848DcCqArgDWAHgo9IZmNsLMis2seO/evZV8OBGpbpVqfndf5+4H3P0ggCcBdCdvO97dC929MLZARUSyp1LNb2ZHbld7DYDw8ikRqZEqMtX3dwAXA2hiZqUA7gVwsZl1BeAAVgL4xTEco4gcA1ldz9+0aVNne+/HzoovLy8PZmeeeSatXbVqFc07depE8y1btgSzZs2a0drY2vHYvv//+c9/aN6yZctgxq4ZAOzZs6dK+c6dO2nO9hqI7bsf+3ro3j340yYAPrazzjqL1r7++us0j329lZaW0rx169bBbOvWrbSWnTExbtw4fPnll1rPLyJhan6RRKn5RRKl5hdJlJpfJFFqfpFEZfWIboAfLxybVmJHUT/++OO0dvjw4TTfuHEjzZctWxbM5s2bR2tPPfVUmo8ePZrmL7/8Ms3ZVs+x7dAnTJhA8xEjRtA8tiX61VdfHcymTJlCa2+66SaaP/PMMzQ/77zzgtlf//pXWvvTn/6U5rFpyPz8fJo/+eSTwaxv3760lk3tfpe7aPXML5IoNb9IotT8IolS84skSs0vkig1v0ii1Pwiicrqkt6CggJn2zHn5eXReja32rZtW1rbsCHfZrAqx2jv2rWL1saOXI5t1VxWVkZzNpf+0Ucf0Vo2Fw4AH3/8Mc3ZfRsAsHr16mAW2/J8+vTpNGfHvQPA/Pnzg1nsmi9atIjm/fr1o/nvfvc7ml922WXBLPb10q5du2A2adIkrF27Vkt6RSRMzS+SKDW/SKLU/CKJUvOLJErNL5IoNb9IorK+dffPfvazYL5y5Upaf/bZZwezWrX4/2Oxef4FCxbQ/Oc//3kwe+mll2gtm4cHgFmzZtE8tna8d+/ela6NbZ/doUMHmo8fP57m7du3D2bs3gkgfrx47HM+cODAYDZo0CBaO2zYMJrXrl2b5rF7Vthx8y1atKC1mzdvDmZTpkzRPL+IcGp+kUSp+UUSpeYXSZSaXyRRan6RRKn5RRIVnec3s1YAJgNoBuAggPHu/qiZNQLwAoC2AFYCGOju4QlIAPn5+T5gwIBgztYpA8Dnn38ezLp06UJrY0cmx/YDWLp0aTDr0aMHrY0d98zuXwCAAwcO0HzHjh3BLHYWQrdu3WheVFRE89i69jlz5gSzK664gtb+7W9/o3nsc8buA+jatSutXbFiBc3btGlD82nTptGc7WUQe2x2XPxLL72E8vLyapvn3w/gdnc/A8B5AG4xs04A7gTwlrufBuCtzL9F5Hsi2vzuvsbdF2Ze3g6gBMApAPoBmJR5s0kA+G1sIlKjfKef+c2sLYCzALwHoKm7rwEO/QcBoKC6Bycix06Fm9/MTgTwMoBb3X3bd6gbYWbFZlYcu5dbRLKnQs1vZnk41PjPuvvUzKvXmVnzTN4cQPnRat19vLsXunth3bp1q2PMIlINos1vh7ZnnQCgxN0fPiKaAeDw9qlDAPyj+ocnIsdKRY7ovhDADQCWmNnh/YzvAnA/gCIzGwbgCwDXxd5RnTp16HLFkpISWs+ObB47diytveiii2j+1Vdf0ZxtYd24cWNae+mll9KcbTENACNHjqT55MmTg9npp59Oa//4xz/SfMyYMTR/7bXXaH7yyScHs9hS6NgR3bHH7tixYzB75ZVXaG1sWe3UqVNpzraoB4AvvvgimLFtvQG+/Py7HNEdbX53fwdAaN6Qj1JEaizd4SeSKDW/SKLU/CKJUvOLJErNL5IoNb9IorK6dXeTJk38qquuCuYtW7ak9Rs2bAhmsSW9MevWraN5fn5+MIvNw48aNYrmsTllNicM8I+dLUUGgF/+8pc0j90/Ebvu7ChstkQbiI/9nHPOoTnb+rtz58609s0336R5bNltVcb2xhtv0NpevXoFsz//+c9YtWqVtu4WkTA1v0ii1PwiiVLziyRKzS+SKDW/SKLU/CKJqsh6/mpTt25dur58+vTptJ7Nb8ZqO3XqRPN///vfNB8yZEgw69mzJ62Nre2eO3cuzWP3CYwbNy6Ysa3SgfjW27GjqtnW3ADQp0+fYBabzx46dCjNd+7cSfNVq1YFs9hx8LE9GmJjv+aaa2j+4IMPBrO+ffvSWrZm/9DeOxWjZ36RRKn5RRKl5hdJlJpfJFFqfpFEqflFEqXmF0lU1tfzX3nllcH8jDPOoPXr168PZu3bt6e127bxE8aOP/54mrO90mPzsvPmzaM5O3IZiB9Fzea72fHdAD8qGoivuY/tg/Dpp58Gs8LCQlobO+Epln/wwQfBrFWrVrSWfb4BoHfv3jSPnQvAjnVnZx0AQHFxcTArKiqq1iO6ReS/kJpfJFFqfpFEqflFEqXmF0mUml8kUWp+kURF1/ObWSsAkwE0A3AQwHh3f9TMxgC4GcDhyfe73J1Obubl5aFZs2bBPDZn3KZNm2AW2+M9dm75q6++SvPbbrstmMX2cGdzugCwceNGmsfGPnPmzGDG9kAAgClTptA8Nuf8ox/9iOb79u0LZvPnz6e1sfs+2Jp4ALj77ruDWWweP3YPwTPPPEPzm2++mebsc15UVERrY/d9VFRFNvPYD+B2d19oZicBeN/MZmWyv7g7/wyISI0UbX53XwNgTebl7WZWAuCUYz0wETm2vtPP/GbWFsBZAN7LvGqkmS02s4lm1jBQM8LMis2sOLbtkohkT4Wb38xOBPAygFvdfRuAcQBOBdAVh74zeOhode4+3t0L3b2QnU8mItlVoeY3szwcavxn3X0qALj7Onc/4O4HATwJoPuxG6aIVLdo89uh7UAnAChx94ePeH3zI97sGgD81+0iUqNEl/Sa2UUA5gFYgkNTfQBwF4BBOPQtvwNYCeAXmV8OBsWO6I5tl8zEpqRWr15N8/r169M8Ly8vmO3evZvWsulNID6Vt2vXLprv378/mG3evJnWxsYWW9J73HH8d8ZsqXWsln1cAJ9GBPjndOHChbS2efPmNO/WrRvNX3vtNZofPHgwmMWOqt+0aVMw+y5Leivy2/53ABztnfEFyyJSo+kOP5FEqflFEqXmF0mUml8kUWp+kUSp+UUSldUjus2MzmkvW7aM1rM54w0bNtDa2Fz6F198QfOuXbsGs9g8PzsquiKPHbsXo2HDoy6rABBfLhzb2rugoIDmsfnyWrXCzy/s3gkgfoz26NGjaT579uxgFlsWW7t2bZo/+uijNL/88stpzm51X7RoEa1l92aw6/2tt63wW4rIfxU1v0ii1PwiiVLziyRKzS+SKDW/SKLU/CKJyuoR3Wa2HsCRk95NAPAJ+typqWOrqeMCNLbKqs6xtXH3/Iq8YVab/1sPblbs7vyQ9hypqWOrqeMCNLbKytXY9G2/SKLU/CKJynXzj8/x4zM1dWw1dVyAxlZZORlbTn/mF5HcyfUzv4jkSE6a38z6mNlyM/vEzO7MxRhCzGylmS0xs0VmVpzjsUw0s3IzW3rE6xqZ2SwzW5H5O7yeN/tjG2NmX2au3SIz+58cja2Vmc0xsxIz+9DMRmden9NrR8aVk+uW9W/7zaw2gI8B9AJQCmABgEHu/lFWBxJgZisBFLp7zueEzewnAL4CMNndu2Re9wCATe5+f+Y/zobufkcNGdsYAF/l+uTmzIEyzY88WRrA1QBuRA6vHRnXQOTguuXimb87gE/c/TN33wvgeQD9cjCOGs/d5wL45gkN/QBMyrw8CYe+eLIuMLYawd3XuPvCzMvbARw+WTqn146MKydy0fynADjy+JxS1Kwjvx3AG2b2vpmNyPVgjqLp4ZORMn/zrXayL3pyczZ942TpGnPtKnPidXXLRfMf7fSfmjTlcKG7nw3gCgC3ZL69lYqp0MnN2XKUk6VrhMqeeF3dctH8pQBaHfHvlgDKcjCOo3L3sszf5QCmoeadPrzu8CGpmb/Lczye/1eTTm4+2snSqAHXriadeJ2L5l8A4DQza2dmdQBcD2BGDsbxLWZWP/OLGJhZfQCXo+adPjwDwJDMy0MA/COHY/mamnJyc+hkaeT42tW0E69zcpNPZirjEQC1AUx09/uyPoijMLP2OPRsDxza2fi5XI7NzP4O4GIcWvW1DsC9AKYDKALQGsAXAK5z96z/4i0wtovxHU9uPkZjC50s/R5yeO2q88TrahmP7vATSZPu8BNJlJpfJFFqfpFEqflFEqXmF0mUml8kUWp+kUSp+UUS9X8VKltb65nPvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, SHAPE_SIZE, SHAPE_SIZE, 1])\n",
    "generated_image = generator(noise, training=False)\n",
    "print(generated_image.shape)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[SHAPE_SIZE, SHAPE_SIZE, 1]))\n",
    "    \n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))    \n",
    "    \n",
    "    #model.add(layers.Conv2D(512, (5, 5), strides=(2, 2), padding='same'))\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #model.add(layers.Conv2D(1024, (5, 5), strides=(2, 2), padding='same'))\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))        \n",
    "    \n",
    "    #model.add(layers.Conv2D(2048, (5, 5), strides=(2, 2), padding='same'))\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #model.add(layers.Conv2D(4096, (5, 5), strides=(2, 2), padding='same'))\n",
    "    #model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n",
      "tf.Tensor([[-0.00011496]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "print(generated_image.shape)\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "noise_dim = 28\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim, noise_dim, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images, masks):\n",
    "    #noise = tf.random.normal([BATCH_SIZE, noise_dim, noise_dim, 1])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(masks, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch, mask_batch in dataset:\n",
    "            train_step(image_batch, mask_batch)\n",
    "\n",
    "        # Produce images for the GIF as we go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,\n",
    "                                 epoch + 1,\n",
    "                                 seed)\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train(ds, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'seg-shapes.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('image*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    last = -1\n",
    "    for i,filename in enumerate(filenames):\n",
    "        #frame = 2*(i**0.5)\n",
    "        #if round(frame) > round(last):\n",
    "        #  last = frame\n",
    "        #else:\n",
    "        #  continue\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "\n",
    "import IPython\n",
    "#if IPython.version_info > (6,2,0,''):\n",
    "display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
